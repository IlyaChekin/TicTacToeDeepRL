{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def init(self):\n",
    "        self.board = np.zeros(9) \n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = np.zeros(9)\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.board\n",
    "    \n",
    "    def available_actions(self):\n",
    "        return [i for i in range(9) if self.board[i] == 0]\n",
    "    \n",
    "    def step(self, action, player):\n",
    "        if self.board[action] != 0:\n",
    "            return self.board, -1, True  # Invalid move\n",
    "        \n",
    "        self.board[action] = player\n",
    "        if self.check_win(player):\n",
    "            self.winner = player\n",
    "            self.done = True\n",
    "            return self.board, 1, self.done  # Player wins\n",
    "        elif np.all(self.board != 0):\n",
    "            self.done = True\n",
    "            return self.board, 0, self.done  # Draw\n",
    "        else:\n",
    "            return self.board, 0, self.done  # Game continues\n",
    "    \n",
    "    def check_win(self, player):\n",
    "        win_conditions = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n",
    "            [0, 4, 8], [2, 4, 6]              # Diagonals\n",
    "        ]\n",
    "        for condition in win_conditions:\n",
    "            if np.all(self.board[condition] == player):\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=9, activation='relu')) \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(9, activation='linear'))  \n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.01))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, model, gamma=0.9, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, batch_size=16, memory_size=10000):\n",
    "        self.model = model\n",
    "        self.gamma = gamma  \n",
    "        self.epsilon = epsilon  \n",
    "        self.epsilon_min = epsilon_min  \n",
    "        self.epsilon_decay = epsilon_decay  \n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []  \n",
    "        self.memory_size = memory_size\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if len(self.memory) >= self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice([i for i in range(9) if state[i] == 0])  \n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)  \n",
    "        return np.argmax(q_values[0])  \n",
    "    \n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Выбор случайной мини-партии из памяти\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, next_state, done in batch:\n",
    "            q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)  # Текущие Q-значения\n",
    "            if done:\n",
    "                q_values[0][action] = reward  # Если игра закончена, то обновляем Q-значение для последнего хода\n",
    "            else:\n",
    "                q_values_next = self.model.predict(np.expand_dims(next_state, axis=0), verbose=0)  # Q-значения следующего состояния\n",
    "                q_values[0][action] = reward + self.gamma * np.max(q_values_next[0])  # Обновление Q-значений\n",
    "            \n",
    "            self.model.fit(np.expand_dims(state, axis=0), q_values, epochs=1, verbose=0)  # Обучение модели\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay  # Уменьшение вероятности случайных действий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]2024-11-22 01:22:50.029829: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, epsilon 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 101/500 [2:29:33<17:39:11, 159.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, epsilon 0.1518722266715875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 145/500 [3:44:08<1:57:37, 19.88s/it]  "
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "agent = DQNAgent(model)\n",
    "env = TicTacToe()\n",
    "\n",
    "episodes = 500 \n",
    "for e in tqdm(range(episodes)):\n",
    "    state = env.reset()  # Сбросить игру\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)  # Выбор хода\n",
    "        next_state, reward, done = env.step(action, 1)  # Совершить ход (игрок 1)\n",
    "        agent.remember(state, action, reward, next_state, done)  # Сохранить опыт\n",
    "        agent.train()  # Обучить модель\n",
    "        state = next_state  # Переход в следующее состояние\n",
    "    if e % 100 == 0:\n",
    "        print(f\"Episode {e}, epsilon {agent.epsilon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'tic_{episodes}eps.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ritosha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
