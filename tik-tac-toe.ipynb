{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, agent1, agent2):\n",
    "        self.board = np.zeros(9, dtype=int)  \n",
    "        self.done = False  \n",
    "        self.agent1 = agent1  \n",
    "        self.agent2 = agent2  \n",
    "        self.current_player = 1  \n",
    "\n",
    "    def reset(self):\n",
    "        self.board = np.zeros(9, dtype=int)\n",
    "        self.done = False\n",
    "        self.current_player = 1\n",
    "        return self.board\n",
    "\n",
    "    def render(self):\n",
    "        symbols = {0: \" \", 1: \"X\", -1: \"O\"}\n",
    "        board = [symbols[cell] for cell in self.board]\n",
    "        print(\"\\n\")\n",
    "        print(f\"{board[0]} | {board[1]} | {board[2]}\")\n",
    "        print(\"--+---+--\")\n",
    "        print(f\"{board[3]} | {board[4]} | {board[5]}\")\n",
    "        print(\"--+---+--\")\n",
    "        print(f\"{board[6]} | {board[7]} | {board[8]}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.board[action] != 0:  # Недопустимый ход\n",
    "            print(\"Invalid move! Penalty applied.\")\n",
    "            return -1  # Штраф за недопустимый ход\n",
    "\n",
    "        self.board[action] = self.current_player\n",
    "        if self.check_winner(self.current_player):  # Проверка на победу\n",
    "            self.done = True\n",
    "            return 1  # Победа\n",
    "        if 0 not in self.board:  # Проверка на ничью\n",
    "            self.done = True\n",
    "            return 0  # Ничья\n",
    "        return 0  # Игра продолжается\n",
    "\n",
    "    def check_winner(self, player):\n",
    "        win_states = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Горизонтали\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Вертикали\n",
    "            [0, 4, 8], [2, 4, 6]              # Диагонали\n",
    "        ]\n",
    "        for line in win_states:\n",
    "            if all(self.board[i] == player for i in line):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def play(self):\n",
    "        self.reset()\n",
    "        self.render()\n",
    "\n",
    "        while not self.done:\n",
    "            agent = self.agent1 if self.current_player == 1 else self.agent2\n",
    "            print(f\"Player {self.current_player}'s turn.\")\n",
    "            action = agent.act(self.board)  \n",
    "            reward = self.step(action)  \n",
    "            self.render()\n",
    "\n",
    "            if reward == 1:\n",
    "                print(f\"Player {self.current_player} wins!\")\n",
    "            elif reward == 0 and self.done:\n",
    "                print(\"It's a draw!\")\n",
    "\n",
    "            if reward != -1:  \n",
    "                self.current_player *= -1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    def act(self, state):\n",
    "        valid_actions = [i for i in range(len(state)) if state[i] == 0]\n",
    "        return random.choice(valid_actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = 'cpu'\n",
    "\n",
    "class TicTacToeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TicTacToeModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        print(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgent():\n",
    "    def __init__(self, model_path):\n",
    "        self.model = torch.load(model_path).to(device)  \n",
    "        self.model.eval()  \n",
    "\n",
    "    def act(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state_tensor)\n",
    "        valid_actions = [i for i in range(len(state)) if state[i] == 0]\n",
    "        valid_q_values = [(i, q_values[0][i].item()) for i in valid_actions]\n",
    "        best_action = max(valid_q_values, key=lambda x: x[1])[0]\n",
    "        return best_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayerAgent():\n",
    "    def act(self, state):\n",
    "        while True:\n",
    "            try:\n",
    "                action = int(input(\"Enter your move (0-8): \"))\n",
    "                if state[action] == 0:\n",
    "                    return action\n",
    "                else:\n",
    "                    print(\"Invalid move. Try again.\")\n",
    "            except (ValueError, IndexError):\n",
    "                print(\"Please enter a valid position (0-8).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  |   |  \n",
      "--+---+--\n",
      "  |   |  \n",
      "--+---+--\n",
      "  |   |  \n",
      "\n",
      "\n",
      "Player 1's turn.\n",
      "tensor([[-0.9785, -1.0860, -0.3907, -0.8634, -0.5995, -0.3748, -0.8533, -0.9156,\n",
      "         -0.1083]])\n",
      "\n",
      "\n",
      "  |   |  \n",
      "--+---+--\n",
      "  |   |  \n",
      "--+---+--\n",
      "  |   | X\n",
      "\n",
      "\n",
      "Player -1's turn.\n",
      "\n",
      "\n",
      "  |   |  \n",
      "--+---+--\n",
      "  | O |  \n",
      "--+---+--\n",
      "  |   | X\n",
      "\n",
      "\n",
      "Player 1's turn.\n",
      "tensor([[36.2807, 24.2059, -5.8762, 20.3418,  4.8915, 19.8901,  6.3150, 21.9055,\n",
      "         19.8494]])\n",
      "\n",
      "\n",
      "X |   |  \n",
      "--+---+--\n",
      "  | O |  \n",
      "--+---+--\n",
      "  |   | X\n",
      "\n",
      "\n",
      "Player -1's turn.\n",
      "\n",
      "\n",
      "X |   |  \n",
      "--+---+--\n",
      "  | O |  \n",
      "--+---+--\n",
      "O |   | X\n",
      "\n",
      "\n",
      "Player 1's turn.\n",
      "tensor([[109.8827, 111.5956, -14.8011, 121.6105,  77.2833, 106.0513, 107.4262,\n",
      "         107.3385, 103.5990]])\n",
      "\n",
      "\n",
      "X |   |  \n",
      "--+---+--\n",
      "X | O |  \n",
      "--+---+--\n",
      "O |   | X\n",
      "\n",
      "\n",
      "Player -1's turn.\n",
      "\n",
      "\n",
      "X |   |  \n",
      "--+---+--\n",
      "X | O | O\n",
      "--+---+--\n",
      "O |   | X\n",
      "\n",
      "\n",
      "Player 1's turn.\n",
      "tensor([[ 89.0414,  85.5587, -20.9227,  90.4197,  66.6956,  77.7539,  70.2654,\n",
      "          75.9974,  75.3911]])\n",
      "\n",
      "\n",
      "X | X |  \n",
      "--+---+--\n",
      "X | O | O\n",
      "--+---+--\n",
      "O |   | X\n",
      "\n",
      "\n",
      "Player -1's turn.\n",
      "\n",
      "\n",
      "X | X |  \n",
      "--+---+--\n",
      "X | O | O\n",
      "--+---+--\n",
      "O | O | X\n",
      "\n",
      "\n",
      "Player 1's turn.\n",
      "tensor([[166.1020, 173.8017,  -9.6716, 196.5777, 124.4099, 171.2661, 181.7044,\n",
      "         174.8212, 169.8762]])\n",
      "\n",
      "\n",
      "X | X | X\n",
      "--+---+--\n",
      "X | O | O\n",
      "--+---+--\n",
      "O | O | X\n",
      "\n",
      "\n",
      "Player 1 wins!\n"
     ]
    }
   ],
   "source": [
    "player2 = RandomAgent()\n",
    "player1 = DeepAgent(r\".\\\\model_100000.pth\")\n",
    "game = TicTacToe(player1, player2)\n",
    "game.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ritosha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
